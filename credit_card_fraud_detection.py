# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SB-1L_XpjBWzQBe_7yBEsud_gNe1Rve7

Importing the Dependencies
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""Uploading dataset as pandas dataframe"""

credit_data=pd.read_csv('/content/creditcard.csv')

# fisrt 5 rows of dataset
credit_data.head()

credit_data.info()

#missing values in dataset in each column
credit_data.isnull().sum()

credit_data['V22'] = credit_data['V22'].fillna(credit_data['V22'].mode()[0])
credit_data['V23'] = credit_data['V23'].fillna(credit_data['V23'].mode()[0])
credit_data['V24'] = credit_data['V24'].fillna(credit_data['V24'].mode()[0])
credit_data['V25'] = credit_data['V25'].fillna(credit_data['V25'].mode()[0])
credit_data['V26'] = credit_data['V26'].fillna(credit_data['V26'].mode()[0])
credit_data['V27'] = credit_data['V27'].fillna(credit_data['V27'].mode()[0])
credit_data['V28'] = credit_data['V28'].fillna(credit_data['V28'].mode()[0])
credit_data['Amount'] = credit_data['Amount'].fillna(credit_data['Amount'].mode()[0])
credit_data['Class'] = credit_data['Class'].fillna(credit_data['Class'].mode()[0])

credit_data.isnull().sum()

credit_data.info()

credit_data['Class'] = credit_data['Class'].astype(int)

# distributuin of legit transactions & raudulent transactions
credit_data['Class'].value_counts()

"""This dataset is highly unbalanced


0 -> Normal Transaction
1 -> Fraudulent transaction
"""

# seperating data
legit = credit_data[credit_data.Class == 0]
fraud = credit_data[credit_data.Class == 1]
print(legit.shape)
print(fraud.shape)

"""Getting Statistical Measures of the data"""

legit.Amount.describe()

fraud.Amount.describe()

#compare the values for both transactions
credit_data.groupby('Class').mean()

#dealing with unbalanced data
#Undersampling
#build a sample dataset containing a similar distribution of normal and fraudulant transactions
# number of fraudulant trnasactions is 262
legit_sample = legit.sample(n=262)

#concatenating two dataframes
new_dataset = pd.concat([legit_sample, fraud], axis=0)

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

new_dataset.groupby('Class').mean()

"""Splitting the data into features & targets"""

X = new_dataset.drop(columns='Class', axis=1)
Y = new_dataset['Class']

print(X)

print(Y)

"""Splitting the data into training and test data sets"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Train the Model

Logistic Regression
"""

model=LogisticRegression()

#training the lofistic regression model
model.fit(X_train, Y_train)

"""Evaluation

Accuracy Score
"""

# accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print(training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print(test_data_accuracy)

"""We got similar accuracy score for training and test datasets."""